{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the articles \n",
    "This notebook uses python3, nltk(stopwords and punkt), newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "import re\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics = [\"Events\", \"FiniActivity\", \"FundRaising\", \"Layoff\", \"MA\", \"Outsourcing\", \"PE\", \"PL\", \"Politics\", \"PP\", \"RandD\"]\n",
    "count_topic = 0\n",
    "for topic in topics[:2]: \n",
    "    print(\"-\"*10)\n",
    "    print(count_topic)\n",
    "    count_topic += 1\n",
    "    output_data_file = open('output/output_data_' + topic + '.csv', 'a')\n",
    "    data = pd.read_csv('data/url_list - ' + topic + '.csv', header=None)\n",
    "    count = 0\n",
    "    outData = []\n",
    "    notFound = []\n",
    "    for index, row in data.iterrows():\n",
    "        print(\"*\"*10)\n",
    "        print(count)\n",
    "        count += 1\n",
    "        try:\n",
    "            d = row[0]\n",
    "            info = {}\n",
    "        #------GETTING THE DATA------\n",
    "            output = Article(d)\n",
    "            output.download()\n",
    "            output.parse()\n",
    "            output = (output.text).encode('utf-8')\n",
    "        #------CLEANING DATA WITH REGEX------\n",
    "            if(output != ''):\n",
    "                output = output.decode('utf-8') #You can get TypeError\n",
    "                output = re.sub(r'\\W+', ' ', output)\n",
    "                output = output.replace('\"', '')\n",
    "                output = output.replace(\"'\", \"\")\n",
    "                output = output.lower()\n",
    "                outData.append(output)\n",
    "        #------OUTPUT DATA------\n",
    "            if(output == ''):\n",
    "                outData.append(\"NA\")\n",
    "                notFound.append(d)\n",
    "                notFound.append(count)\n",
    "            output_data_file.write((outData))\n",
    "        except:\n",
    "            print(\"ERROR!\") #Occationaly link might not be working so download() would fail returning 404 error.\n",
    "    output_data_file.write((outData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the frequent words in the kind of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\"Events\", \"FiniActivity\", \"FundRaising\", \"Layoff\", \"MA\", \"Outsourcing\", \"PE\", \"PL\", \"Politics\", \"PP\", \"RandD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_topic = 0\n",
    "for topic in topics[:2]: \n",
    "    try:\n",
    "        print(\"-\"*10)\n",
    "        print(count_topic)\n",
    "        count_topic += 1\n",
    "        input_data_file = open('output/output_data_' + topic + '.csv').read()\n",
    "        input_data_file = pd.DataFrame(eval(input_data_file))\n",
    "        count = 0\n",
    "        outData = []\n",
    "        notFound = []\n",
    "        for index, row in input_data_file.iterrows():\n",
    "            data = row[0]\n",
    "            print(\"Data Type : \" + str(type(data)))\n",
    "            data = word_tokenize(data)\n",
    "            t_data = [i for i in data if i not in stop_words and len(i) > 2]\n",
    "            token_data = [i for i in t_data if i.isalpha()]\n",
    "            data_pd = pd.DataFrame(token_data)\n",
    "            #-----------------------------------\n",
    "            print(data_pd[0].describe())\n",
    "            print(set(data_pd[0].value_counts()))\n",
    "\n",
    "            #-----------------------------------\n",
    "            df = pd.DataFrame(data_pd[0].value_counts())\n",
    "            print(df)\n",
    "            try:\n",
    "                df.to_csv(\"/unigramFrequency/uf_\" + topic+\".csv\", sep=',')\n",
    "            except:\n",
    "                print(\"File/Folder not found\")\n",
    "\n",
    "            print(\"-\"*100)\n",
    "\n",
    "            #-----------------------------------\n",
    "            bi_words = []\n",
    "            for i in range(0,len(token_data)):\n",
    "                bi_words.append(token_data[i-1] + \" \" + token_data[i])\n",
    "\n",
    "            pd_bi_words = pd.DataFrame(bi_words)\n",
    "\n",
    "            print(pd_bi_words[0].describe())\n",
    "\n",
    "            print(set(pd_bi_words[0].value_counts()))\n",
    "\n",
    "            df = pd.DataFrame(pd_bi_words[0].value_counts())\n",
    "            print(df)\n",
    "            try:\n",
    "                df.to_csv(\"/bigramFrequency/bf_\" + topic+\".csv\", sep=',')\n",
    "            except:\n",
    "                print(\"File/Folder not found\")\n",
    "    except:\n",
    "          print(\"Error, may be no data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
